{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 🧠 Topic Modeling on COVID-19 Tweets Using NMF\n",
                "\n",
                "**Author:** Yiling Xu  \n",
                "**Last Updated:** Sept 2025  \n",
                "**Status:** Ongoing exploration | NLP | Topic Modeling  \n",
                "\n",
                "---\n",
                "\n",
                "## 📌 Project Description\n",
                "\n",
                "This notebook explores the use of **unsupervised learning** to uncover hidden themes in a large-scale Twitter dataset about the COVID-19 pandemic.  \n",
                "Using **TF-IDF vectorization** and **Non-negative Matrix Factorization (NMF)**, we project tweet content into a low-dimensional semantic space and extract **interpretable topics**.\n",
                "\n",
                "---\n",
                "\n",
                "## 💡 Key Steps:\n",
                "- Load and clean ~100,000 English-language tweets from April 30, 2020\n",
                "- Vectorize text using TF-IDF\n",
                "- Apply NMF to extract latent topics\n",
                "- Visualize tweet distributions and outlier content in topic space\n",
                "\n",
                "---\n",
                "\n",
                "## Dataset:\n",
                "\n",
                "The dataset is obtained from [Kaggle](https://www.kaggle.com/smid80/coronavirus-covid19-tweets-late-april?select=2020-04-30+Coronavirus+Tweets.CSV) and the preprocessing we have done followed the steps [here](https://www.kaggle.com/satanizer/covid-19-tweets-analysis). \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup & Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mRunning cells with 'Python 3.12.0' requires the ipykernel package.\n",
                        "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "# Reproducibility\n",
                "np.random.seed(416)\n",
                "\n",
                "# Load dataset (expects a CSV with at least a 'text' column; optional 'lang' column)\n",
                "df = pd.read_csv(\"tweets-2020-4-30.csv\")\n",
                "df = df.copy()\n",
                "df[\"text\"] = df[\"text\"].fillna(\"\")\n",
                "df.tail()\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 🧹 Preprocessing\n",
                "\n",
                "The original tweets have been pre-cleaned to support topic modeling:\n",
                "- Filtered for English language only\n",
                "- Removed URLs, punctuation, and common COVID hashtags\n",
                "- Lowercased and stopword-removed using `nltk`\n",
                "\n",
                "The cleaned text is stored in the `text` column.\n",
                "\n",
                "```python\n",
                "import re, string\n",
                "import pandas as pd\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "\n",
                "# Make sure NLTK resources are available\n",
                "nltk.download(\"stopwords\")\n",
                "\n",
                "# Filter to English if the column exists\n",
                "if \"lang\" in df.columns:\n",
                "    df = df.loc[df[\"lang\"].astype(str).str.lower() == \"en\"].copy()\n",
                "\n",
                "# Basic cleaning functions\n",
                "URL_PATTERN = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
                "\n",
                "def strip_urls(s: str) -> str:\n",
                "    return URL_PATTERN.sub(\"\", s)\n",
                "\n",
                "def strip_punct(s: str) -> str:\n",
                "    return s.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
                "\n",
                "# Stopwords & domain-specific high-frequency tokens\n",
                "stop_words = set(stopwords.words(\"english\"))\n",
                "stop_words.update([\n",
                "    \"#coronavirus\", \"#coronavirusoutbreak\", \"#coronaviruspandemic\",\n",
                "    \"#covid19\", \"#covid_19\", \"#epitwitter\", \"#ihavecorona\",\n",
                "    \"amp\", \"coronavirus\", \"covid19\", \"covid-19\", \"covidー19\"\n",
                "])\n",
                "\n",
                "def remove_stopwords(s: str) -> str:\n",
                "    return \" \".join(w for w in s.split() if w not in stop_words)\n",
                "\n",
                "# Apply cleaning\n",
                "clean = (\n",
                "    df[\"text\"]\n",
                "    .astype(str)\n",
                "    .str.lower()\n",
                "    .apply(strip_urls)\n",
                "    .apply(strip_punct)\n",
                "    .apply(remove_stopwords)\n",
                ")\n",
                "\n",
                "df[\"text_clean\"] = clean\n",
                "df[\"text_clean\"].head()\n",
                "```\n",
                "\n",
                "## TF-IDF Matrix\n",
                "\n",
                "Convert cleaned text into a sparse document–term matrix via TF‑IDF."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(119147, 183012)"
                        ]
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "vectorizer = TfidfVectorizer(max_df=0.95)  # ignore extremely common terms\n",
                "tf_idf = vectorizer.fit_transform(df[\"text_clean\"])  # shape: (num_docs, num_terms)\n",
                "feature_names = vectorizer.get_feature_names_out()\n",
                "\n",
                "num_tweets, num_words = tf_idf.shape\n",
                "print(f\"TF‑IDF shape: {tf_idf.shape}  | tweets={num_tweets}, terms={num_words}\")\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 🧩 Topic Modeling with NMF\n",
                "\n",
                "Use Non‑negative Matrix Factorization (NMF) to uncover latent themes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from sklearn.decomposition import NMF\n",
                "\n",
                "# 5 topics for interpretability; adjust n_components as needed\n",
                "nmf = NMF(n_components=5, init=\"nndsvd\", random_state=416)\n",
                "tweets_projected = nmf.fit_transform(tf_idf)   # (num_docs, n_topics)\n",
                "components = nmf.components_                   # (n_topics, num_terms)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 🔑 Top Words per Topic\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "def words_from_topic(topic_row: np.ndarray, vocab: np.ndarray, top_k: int = 10):\n",
                "    idx = np.argsort(topic_row)[::-1][:top_k]\n",
                "    return [vocab[i] for i in idx]\n",
                "\n",
                "for i, row in enumerate(components):\n",
                "    top_words = \", \".join(words_from_topic(row, feature_names, top_k=10))\n",
                "    print(f\"Topic #{i}: {top_words}\")\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 🔎 Inspect a Sample Tweet\n",
                "\n",
                "Check a specific tweet’s topic weights (use a safe index)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['cats', 'axolotl', 'dogs']"
                        ]
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "idx = min(40151, len(df) - 1)  # safe-guard if dataset is smaller\n",
                "print(\"Tweet:\", df.iloc[idx][\"text\"])\n",
                "print(\"Cleaned:\", df.iloc[idx][\"text_clean\"])\n",
                "print(\"Topic weights:\", tweets_projected[idx])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 📊 Dominant Topic Distribution\n",
                "\n",
                "Compute each tweet’s dominant topic and find the most frequent one."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dom = np.argmax(tweets_projected, axis=1)         # dominant topic per tweet\n",
                "largest_topic = int(np.bincount(dom).argmax())    # most common dominant topic\n",
                "print(f\"Most frequent dominant topic: {largest_topic}\")\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 📈 3D Visualization (NMF with 3 Topics)\n",
                "\n",
                "Project into 3 topics to enable a 3D scatter visualization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "attention seattle shoppers grocery stores working hard keep employees customers safe part help slow spread ☑️ limit trips ☑️ respect special shopping hours ☑️ follow socialdistance guidance stores wegotthisseattle\n",
                        "[0.00823661 0.         0.02895533 0.         0.01529455]\n"
                    ]
                }
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 (required for 3D projection)\n",
                "\n",
                "nmf_small = NMF(n_components=3, init=\"nndsvd\", random_state=416)\n",
                "proj3 = nmf_small.fit_transform(tf_idf)  # (num_docs, 3)\n",
                "\n",
                "fig = plt.figure(figsize=(6, 6))\n",
                "ax = fig.add_subplot(projection=\"3d\")\n",
                "ax.scatter(proj3[:, 0], proj3[:, 1], proj3[:, 2], s=2, alpha=0.5)\n",
                "ax.set_xlabel(\"Topic 0\"); ax.set_ylabel(\"Topic 1\"); ax.set_zlabel(\"Topic 2\")\n",
                "ax.set_title(\"Tweets in 3D Topic Space (NMF, k=3)\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Outlier Tweets in Topic Space\n",
                "\n",
                "Identify tweets strongly aligned with Topic 2 in the 3‑topic model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "threshold = 0.15\n",
                "mask = proj3[:, 2] >= threshold\n",
                "outlier_tweets = df.loc[mask, \"text_clean\"].drop_duplicates().to_numpy()\n",
                "print(f\"Outlier tweets (unique): {len(outlier_tweets)}\")\n",
                "outlier_tweets[:10]  # preview\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
